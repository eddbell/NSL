perceptrons: given several inputs with different weight it return an output which can be 0 or 1.
sigmoid neurons with activation functions: from the same previous idea they return value less change-dependent wich are a continuum using funcitons like: Sigmoid.
 - Itâ€™s the smoothness of the activation functions is a crucial fact: small changes in the weights and in the bias will produce a small change output
 
 the accuracy actually is the loss done from the validation dataset.
 When in DNN we define accuracy we meant something else: a metric of performance defined like the percentage of correctly classified data points
 
 
 
 ps: ex 11 cambiare accuracy in validation/set dataset
