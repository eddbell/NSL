{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-06 12:15:47.813446: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-06 12:15:47.851414: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-06 12:15:47.851872: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-06 12:15:48.518747: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.environ['KMP_DUPLICATE_LIB_OK']='True' #This is needed in my Anaconda+MacOsX installation; leave it commented.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist  #carico le foto dal dataset di keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD, Adam, RMSprop, Adagrad, Adadelta, Adamax, Nadam\n",
    "plt.rcParams.update({'font.size': 20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\"> Numerical Simulation Laboratory </span>\n",
    "## <span style=\"color:brown\"> Python Exercise 12 </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12: MNIST with Keras\n",
    "\n",
    "The goal of exercise 12 is to use deep neural network models, implemented in the Keras python package, to recognize and distinguish between the ten handwritten digits (0-9).\n",
    "\n",
    "The MNIST dataset comprises $70000$ handwritten digits, each of which comes in a square image, divided into a $28\\times 28$ pixel grid. Every pixel can take on $256$ gradation of the gray color, interpolating between white and black, and hence each data point assumes any value in the set $\\{0,1,\\dots,255\\}$. Since there are $10$ categories in the problem, corresponding to the ten digits, this problem represents a generic **classification task**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and Process the Data\n",
    "\n",
    "We load the MNIST data from the web using `load_data()` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 28, 28)\n",
      "Y_train shape: (60000,)\n"
     ]
    }
   ],
   "source": [
    "seed=0\n",
    "np.random.seed(seed) # fix random seed\n",
    "tf.random.set_seed(seed)\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28 # number of pixels \n",
    "# output\n",
    "num_classes = 10 # 10 digits\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('Y_train shape:', Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Once we have loaded the data, we need to format it in the correct shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 784)\n",
      "X_test shape: (10000, 784)\n",
      "\n",
      "an example of a data point with label 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... and with label [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] after to_categorical\n",
      "\n",
      "X_train shape: (60000, 784)\n",
      "Y_train shape: (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "# reshape data, it could depend on Keras backend\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows*img_cols)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows*img_cols)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print()\n",
    "\n",
    "# cast floats to single precision\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# rescale data in interval [0,1]\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# look at an example of data point\n",
    "print('an example of a data point with label', Y_train[20])\n",
    "# matshow: display a matrix in a new figure window\n",
    "plt.matshow(X_train[20,:].reshape(28,28),cmap='binary')\n",
    "plt.show()\n",
    "\n",
    "# convert class vectors to binary class matrices, e.g. for use with categorical_crossentropy\n",
    "Y_train = keras.utils.to_categorical(Y_train, num_classes)\n",
    "Y_test = keras.utils.to_categorical(Y_test, num_classes)\n",
    "print('... and with label', Y_train[20], 'after to_categorical')\n",
    "print()\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('Y_train shape:', Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.1\n",
    "\n",
    "The neural network consists of 3 layers:\n",
    "\n",
    "- 400 neurons with **ReLU** activation function.\n",
    "- 100 neurons with **ReLU** activation function.\n",
    "- Output layer with 10 neurons and **softmax** activation function.\n",
    "\n",
    "The last layer must be limited to map the output to the 10 digits.\n",
    "\n",
    "Furthermore, we introduce a 50% dropout, which is a procedure where a portion of the neurons is deactivated for each epoch. This is useful to prevent overfitting, a phenomenon in which the model memorizes individual training data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_DNN():\n",
    "    # instantiate model\n",
    "    model = Sequential()\n",
    "    # add a dense all-to-all relu layer\n",
    "    model.add(Dense(400,input_shape=(img_rows*img_cols,), activation='relu'))\n",
    "    # add a dense all-to-all relu layer\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    # apply dropout with rate 0.5\n",
    "    model.add(Dropout(0.5))\n",
    "    # soft-max layer\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    print('Model architecture created successfully!')\n",
    "    return model\n",
    "\n",
    "def CaCModel(opt):\n",
    "    # create the model\n",
    "    model=create_DNN()\n",
    "    # compile the model\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=opt,\n",
    "                  metrics=['acc'])\n",
    "    model.save_weights(f'weights/model_weights_{opt}.h5')\n",
    "    print('Model compiled successfully and ready to be trained.')\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We begin addressing the problem by studying the performance variation concerning the number of epochs the system has been trained for.\n",
    "\n",
    "In order to adjusting the model as necessary and we extend the training duration by increasing the number of epochs from 5 to 15. Additionally, we experiment the Deep Neural Network with different optimizers:\n",
    "\n",
    "- Stochastic Gradient Descent (SGD).\n",
    "- (RMSprop)\n",
    "- (Adam)\n",
    "\n",
    "While doing so, it's crucial to monitor the accuracy and loss metrics for both the training and validation datasets. So we provide commentary on the model's performance under different optimizer choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW MODEl with optimizer  sgd\n",
      "Model architecture created successfully!\n",
      "Model compiled successfully and ready to be trained.\n",
      "Epoch 1/15\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.7785 - acc: 0.7703 - val_loss: 0.3145 - val_acc: 0.9136\n",
      "Epoch 2/15\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3851 - acc: 0.8899 - val_loss: 0.2407 - val_acc: 0.9306\n",
      "Epoch 3/15\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3129 - acc: 0.9115 - val_loss: 0.2086 - val_acc: 0.9389\n",
      "Epoch 4/15\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2699 - acc: 0.9239 - val_loss: 0.1800 - val_acc: 0.9467\n",
      "Epoch 5/15\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2410 - acc: 0.9319 - val_loss: 0.1634 - val_acc: 0.9508\n",
      "Epoch 6/15\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2163 - acc: 0.9381 - val_loss: 0.1463 - val_acc: 0.9571\n",
      "Epoch 7/15\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1930 - acc: 0.9442 - val_loss: 0.1358 - val_acc: 0.9578\n",
      "Epoch 8/15\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1807 - acc: 0.9491 - val_loss: 0.1262 - val_acc: 0.9602\n",
      "Epoch 9/15\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1662 - acc: 0.9523 - val_loss: 0.1180 - val_acc: 0.9631\n",
      "Epoch 10/15\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1563 - acc: 0.9558 - val_loss: 0.1114 - val_acc: 0.9657\n",
      "Epoch 11/15\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1445 - acc: 0.9588 - val_loss: 0.1073 - val_acc: 0.9666\n",
      "Epoch 12/15\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1377 - acc: 0.9606 - val_loss: 0.1012 - val_acc: 0.9685\n",
      "Epoch 13/15\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1280 - acc: 0.9631 - val_loss: 0.0956 - val_acc: 0.9696\n",
      "Epoch 14/15\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1209 - acc: 0.9652 - val_loss: 0.0912 - val_acc: 0.9714\n",
      "Epoch 15/15\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1155 - acc: 0.9670 - val_loss: 0.0891 - val_acc: 0.9715\n",
      "NEW MODEl with optimizer  RMSprop\n",
      "Model architecture created successfully!\n",
      "Model compiled successfully and ready to be trained.\n",
      "Epoch 1/15\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.2817 - acc: 0.9177 - val_loss: 0.1275 - val_acc: 0.9649\n",
      "Epoch 2/15\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.1421 - acc: 0.9639 - val_loss: 0.1204 - val_acc: 0.9691\n",
      "Epoch 3/15\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1223 - acc: 0.9707 - val_loss: 0.1096 - val_acc: 0.9745\n",
      "Epoch 4/15\n",
      " 773/1875 [===========>..................] - ETA: 3s - loss: 0.1181 - acc: 0.9717"
     ]
    }
   ],
   "source": [
    "# training parameters\n",
    "batch_size = 32\n",
    "epochs = 15\n",
    "optimizers = ('sgd','RMSprop','Nadam','Adagrad')\n",
    "losses=[]\n",
    "val_losses=[]\n",
    "accs=[]\n",
    "val_accs=[]\n",
    "for opt in optimizers:\n",
    "    print(\"NEW MODEl with optimizer \",opt)\n",
    "    # create the deep neural net\n",
    "    model_DNN = CaCModel(opt)\n",
    "\n",
    "    # train DNN and store training info in history\n",
    "    history = model_DNN.fit(X_train, Y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              validation_data=(X_test, Y_test))\n",
    "    \n",
    "    losses.append(history.history['loss'])\n",
    "    val_losses.append(history.history['val_loss'])\n",
    "    accs.append(history.history['acc'])\n",
    "    val_accs.append(history.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look into training history\n",
    "plt.subplots(figsize=(18,15))\n",
    "plt.subplot(2,1,1)\n",
    "for i in range(len(optimizers)):\n",
    "    plt.plot(losses[i],linewidth = 4,label=f\"Train {optimizers[i]}\")\n",
    "    #plt.plot(val_losses[i],linewidth = 4,label=f\"Validation {optimizers[i]}\")\n",
    "plt.title('Model loss', fontsize=16)\n",
    "plt.grid()\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs [#]')\n",
    "plt.legend( loc='best')\n",
    "\n",
    "# summarize history for loss\n",
    "plt.subplot(2,1,2)\n",
    "plt.title('Model Accuracy', fontsize=16)\n",
    "for i in range(len(optimizers)):\n",
    "    plt.plot(accs[i],linewidth = 4,label=f\"Train {optimizers[i]}\")\n",
    "    #plt.plot(val_accs[i],linewidth = 4,label=f\"Validation {optimizers}\")\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs [#]')\n",
    "plt.grid()\n",
    "plt.legend( loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sgd Optimizer have the lowest loss function along the epochs. It reachs the minimum when the Adam optimizator already did. On the other hand the RMSprop reach the the minimum loss value after only 2 epochs but then araise again. That's probably due to the overfitting problem and maybe the dropout acts less well for the selected optimizer. The optimizer the minimize the loss funciton and maximaze the efficiency is the Nadam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will shows the validation tests for the previous DNN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look into training history\n",
    "plt.subplots(figsize=(18,15))\n",
    "plt.subplot(2,1,1)\n",
    "for i in range(len(optimizers)):\n",
    "    #plt.plot(losses[i],linewidth = 4,label=f\"Train {optimizers}\")\n",
    "    plt.plot(val_losses[i],linewidth = 4,label=f\"Validation {optimizers[i]}\")\n",
    "plt.title('Model loss', fontsize=16)\n",
    "plt.grid()\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs [#]')\n",
    "plt.legend( loc='best')\n",
    "\n",
    "# summarize history for loss\n",
    "plt.subplot(2,1,2)\n",
    "plt.title('Model Accuracy', fontsize=16)\n",
    "for i in range(len(optimizers)):\n",
    "    #plt.plot(accs[i],linewidth = 4,label=f\"Train {optimizers[i]}\")\n",
    "    plt.plot(val_accs[i],linewidth = 4,label=f\"Validation {optimizers[i]}\")\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs [#]')\n",
    "plt.grid()\n",
    "plt.legend( loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "\n",
    "\n",
    "High accuracy and low loss function value are confirmed for the **Nadam** optimizer.\n",
    "The number of epochs can then be reduced to the resonable number of **8 epochs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MODEl with optimizer Nadam\")\n",
    "# create the deep neural net\n",
    "epochs = 8\n",
    "model_DNN = CaCModel('Nadam')\n",
    "\n",
    "# train DNN and store training info in history\n",
    "history = model_DNN.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test))\n",
    "    \n",
    "predictions = model_DNN.predict(X_test)\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols,1)\n",
    "\n",
    "plt.figure(figsize=(23, 15)) \n",
    "for i in range(15):    \n",
    "    ax = plt.subplot(2, 15, i + 1)    \n",
    "    plt.imshow(X_test[i, :, :, 0], cmap='gray')    \n",
    "    plt.title(\"Digit: {}\\nPredicted: {}\".format(np.argmax(Y_test[i]), np.argmax(predictions[i])))    \n",
    "    plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "The Parameter found respond with high accuracy and low loss value after only 8 epochs.\n",
    "The test above shows that the model is good enought to predict the 10 digits above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.2 : Creating Convolutional Neural Nets (CNN)\n",
    "\n",
    "Now we would like to improve the accuracy of our model exploiting the *translational invariance* of the totality of the pixels and the *local* spatial correlations between the pixels. This is possible introducing Convulutional neural nets.\n",
    "\n",
    "To face it, we first need to reshape the training and test input data as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you will need the following for Convolutional Neural Networks\n",
    "from keras.layers import Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "# reshape data, depending on Keras backend\n",
    "#serve a portarlo nel formato della figura\n",
    "if keras.backend.image_data_format() == 'channels_first':\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "    \n",
    "print('X_train shape:', X_train.shape)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print()\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the architecture of the DNN model using convolutional layers. We consider:\n",
    "\n",
    "- 10 neurons for the **Conv2D** with **ReLU** activation function.\n",
    "- 10 neurons for the **MaxPooling2D** with **ReLU** activation function. \n",
    "- 5 neurons for the **Conv2D** with **ReLU** activation function.\n",
    "\n",
    "Then the DNN before defined:\n",
    "- 400 neurons with **ReLU** activation function.\n",
    "- 100 neurons with **ReLU** activation function.\n",
    "- Output layer with 10 neurons and **softmax** activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_CNN():\n",
    "    # instantiate model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(10, kernel_size=(5, 5), activation='relu',input_shape=input_shape))                \n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(5,(5,5), activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(400, activation='relu'))\n",
    "    # add a dense all-to-all relu layer\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    # apply dropout with rate 0.5\n",
    "    model.add(Dropout(0.5))  \n",
    "    # soft-max layer\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer='Nadam',\n",
    "                  metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "batch_size = 32\n",
    "epochs = 8\n",
    "\n",
    "# create the deep conv net\n",
    "model_CNN=create_CNN()\n",
    "\n",
    "# train CNN\n",
    "history = model_CNN.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test))\n",
    "\n",
    "# evaliate model\n",
    "score = model_CNN.evaluate(X_test, Y_test, verbose=1)\n",
    "\n",
    "# print performance\n",
    "print()\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look into training history\n",
    "plt.subplots(figsize=(18,15))\n",
    "plt.subplot(2,1,1)\n",
    "\n",
    "plt.plot(history.history['loss'],linewidth = 4,label=\"Train\")\n",
    "plt.plot(history.history['val_loss'],linewidth = 4,label=\"Validation \")\n",
    "plt.title('Model loss', fontsize=16)\n",
    "plt.grid()\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs [#]')\n",
    "plt.legend( loc='best')\n",
    "\n",
    "# summarize history for loss\n",
    "plt.subplot(2,1,2)\n",
    "plt.title('Model Accuracy', fontsize=16)\n",
    "\n",
    "plt.plot(history.history['acc'],linewidth = 4,label=\"Train\")\n",
    "plt.plot(history.history['val_acc'],linewidth = 4,label=\"Validation \")\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs [#]')\n",
    "plt.grid()\n",
    "plt.legend( loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test = X_test.reshape(X_test.shape[0], img_rows*img_cols)\n",
    "predictions = model_CNN.predict(X_test)\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols,1)\n",
    "\n",
    "plt.figure(figsize=(23, 15)) \n",
    "for i in range(15):    \n",
    "    ax = plt.subplot(2, 15, i + 1)    \n",
    "    plt.imshow(X_test[i, :, :, 0], cmap='gray')    \n",
    "    plt.title(\"Digit: {}\\nPredicted: {}\".format(np.argmax(Y_test[i]), np.argmax(predictions[i])))    \n",
    "    plt.axis('off') \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "We introduced the new convolution model wich perform well also if it reduce the accuracy of the DNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.3\n",
    "\n",
    "Let's try with 10 pictures of mine \"handwritten\" digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "datas=[]\n",
    "predictions=[]\n",
    "filenames = {\"one\",\"two\",\"three\",\"four\",\"five\",\"six\",\"seven\",\"eight\",\"nine\"}\n",
    "\n",
    "for name in filenames:\n",
    "    digit_filename = f\"./mydigits/{name}.png\"\n",
    "    digit_in = Image.open(digit_filename).convert('L')\n",
    "\n",
    "    ydim, xdim = digit_in.size\n",
    "    pix=digit_in.load();\n",
    "    data = np.zeros((xdim, ydim))\n",
    "    for j in range(ydim):\n",
    "        for i in range(xdim):\n",
    "            data[i,j]=pix[j,i]\n",
    "    data /= 255\n",
    "    data = data.reshape(1,xdim*ydim)\n",
    "    pred_0 = model_DNN.predict(data)\n",
    "    predictions.append(pred_0)\n",
    "    data = data.reshape(xdim,ydim)\n",
    "    datas.append(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the previously trained DNN to predict the digit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(23, 15)) \n",
    "for i in range(9):    \n",
    "    ax = plt.subplot(2, 15, i + 1)    \n",
    "    plt.imshow(datas[i], cmap='gray')    \n",
    "    plt.title(f\"Predicted: {np.argmax(predictions[i])}\" )   \n",
    "    plt.axis('off') \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "The CNN model predict well all the digits look-likes training ones. For example the 4 been written in an other way is not predicted. That is, the neural network is incapable of predicting data in areas outside the training range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
